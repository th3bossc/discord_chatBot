{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W7Cp4Gyfjrsl"
      },
      "outputs": [],
      "source": [
        "TOKEN = 'TOKEN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Waf1b96glGZx"
      },
      "outputs": [],
      "source": [
        "#discord integration\n",
        "!pip install discord\n",
        "import discord\n",
        "intents = discord.Intents.default()\n",
        "intents.message_content = True\n",
        "client = discord.Client(intents = intents)\n",
        "\n",
        "#bot model\n",
        "import torch\n",
        "!pip install transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelWithLMHead, TrainingArguments, Trainer, DataCollatorWithPadding, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "387z8YJg_1qF"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Th3BossC/DialoGPT-medium-AICLUB_NITC\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Th3BossC/DialoGPT-medium-AICLUB_NITC\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "from pathlib import Path\n",
        "save_path = Path(\"models/\")\n",
        "save_path.mkdir(parents = True, exist_ok = True)\n",
        "torch.save(model, save_path / 'DiscordBotDioloGPTmedium.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "14PlLJC8-S1S"
      },
      "outputs": [],
      "source": [
        "class Bot():\n",
        "  def __init__(self):\n",
        "    self.step = 0\n",
        "    self.chat_history_ids : torch.LongTensor = []\n",
        "\n",
        "  def create_bot_ids(self, usr):\n",
        "    new_user_input_ids = tokenizer.encode(usr + tokenizer.eos_token, return_tensors = \"pt\")\n",
        "    if self.step == 0:\n",
        "      return new_user_input_ids\n",
        "    else:\n",
        "      return torch.cat([self.chat_history_ids, new_user_input_ids], dim = -1)\n",
        "    \n",
        "\n",
        "  def reply(self, usr):\n",
        "    if usr.lower() == \"bot power off\":\n",
        "      return \"-1\"\n",
        "    #new_user_input_ids = tokenizer.encode(usr + tokenizer.eos_token, return_tensors = \"pt\")\n",
        "    # bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim = -1) if self.step > 0 else new_user_input_ids\n",
        "    bot_input_ids = self.create_bot_ids(usr = usr)\n",
        "    self.chat_history_ids = model.generate(\n",
        "      bot_input_ids, \n",
        "      max_length = 500, \n",
        "      pad_token_id = tokenizer.eos_token_id, \n",
        "      no_repeat_ngram_size = 3, \n",
        "      do_sample = True, \n",
        "      top_k = 100, \n",
        "      top_p = 0.7, \n",
        "      temperature = 0.8\n",
        "    )\n",
        "    self.step += 1\n",
        "    return tokenizer.decode(self.chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens = True)\n",
        "\n",
        "ai_chatbot = Bot()\n",
        "\n",
        "# step = 0\n",
        "# def reply(usr):\n",
        "#   if (usr.lower() == \"bot power off\"):\n",
        "#     return \"-1\"\n",
        "#   new_user_input_ids = tokenizer.encode(usr + tokenizer.eos_token, return_tensors = \"pt\")\n",
        "#   bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim = -1) if step > 0 else new_user_input_ids\n",
        "#   chat_history_ids = model.generate (\n",
        "#       bot_input_ids,\n",
        "#       max_length = 500,\n",
        "#       pad_token_id = tokenizer.eos_token_id,\n",
        "#       no_repeat_ngram_size = 3,\n",
        "#       do_sample = True,\n",
        "#       top_k = 100,\n",
        "#       top_p = 0.7,\n",
        "#       temperature = 0.8\n",
        "#   )\n",
        "#   step += 1\n",
        "#   return tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "riCRmxwDlGnq"
      },
      "outputs": [],
      "source": [
        "@client.event\n",
        "async def on_ready():\n",
        "    print(f\"We have logged in as {client.user}\")\n",
        "\n",
        "@client.event\n",
        "async def on_message(message):\n",
        "    username = str(message.author).split('#')[0]\n",
        "    user_message = str(message.content)\n",
        "    channel = str(message.channel.name)\n",
        "    print(f\"{username}: {user_message} ({channel})\")\n",
        "\n",
        "    if message.author == client.user:\n",
        "        return\n",
        "    if message.content.startswith(\"shutdown\"):\n",
        "        await client.close()\n",
        "    bot_answer = ai_chatbot.reply(usr = user_message)\n",
        "    if message.channel.name == 'talk-with-the-bot':\n",
        "        if bot_answer == \"-1\":\n",
        "            return\n",
        "        else:\n",
        "            await message.channel.send(bot_answer)\n",
        "            return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww6k4QpxlGum"
      },
      "outputs": [],
      "source": [
        "!pip install nest_asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "client.run(TOKEN)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
